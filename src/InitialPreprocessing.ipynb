{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546a6b2e-cc09-4b2f-9e11-dccb9ae922c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import boto3\n",
    "from langdetect import detect\n",
    "from collections import Counter\n",
    "from process_helper import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14deee3-e2da-4a00-a6ce-25ad7d80bdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb35632-8dc5-4390-9e8a-09f376f7434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_excel('news_excerpts_parsed.xlsx')\n",
    "leaks = pd.read_excel('wikileaks_parsed.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecea9c9a-8f96-4a92-97e9-682f513ae871",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks['Text'] = leaks['Text'].astype(str)\n",
    "news['Text'] = news['Text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abd073e-1978-46d4-bf24-d0b0b77e66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ee0ab-30a0-49e5-9313-57945f4efbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574c7f29-1dbb-4d6f-b18b-151e3ba3e2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks['Text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e308fa1-431a-481e-9760-e51494859e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_patterns(df, patterns):\n",
    "    results = {}\n",
    "    for pattern in patterns:\n",
    "        contains_pattern = df['Text'].str.contains(pattern, na=False)\n",
    "        results[pattern] = contains_pattern.sum()  # Count rows containing the pattern\n",
    "    return results\n",
    "\n",
    "# Patterns to check for\n",
    "patterns_to_check = ['\\n', '\\xad', '\\t', 'ï¿½', r'\\\\']\n",
    "\n",
    "# Check both datasets for the patterns\n",
    "print(f\"News : {check_for_patterns(news, patterns_to_check)}\")\n",
    "print(f\"Leaks : {check_for_patterns(leaks, patterns_to_check)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a6490-28b4-4328-8b41-179f96d933b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks['Text'] = leaks['Text'].str.replace('\\n', '.', regex=True)\n",
    "news['Text'] = news['Text'].str.replace('\\n', '.', regex=True)\n",
    "leaks['Text'] = leaks['Text'].str.replace('\\xad', '-', regex=True)\n",
    "leaks['Text'] = leaks['Text'].str.replace( r'\\\\', \"''\", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ccc027-1508-422f-84ff-9771f2491ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec4bca6-3a71-438a-8798-eb9ab7b1b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks['Text'][52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee8f9a-33b5-4bf9-8167-f9b22e18678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks = leaks.groupby('PDF Path', as_index=False).agg({\n",
    "    'Text': ' '.join  \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54affb1-077c-4e2e-8f33-2e59188c017d",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90dbd01-b1c4-45ef-9323-5b9caca4afa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad47c19-9967-4fdb-b406-48b83d81124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks['Token_Count'] = leaks['Text'].apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cf0f4-4748-4c04-a280-9ce3e40505f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['Token_Count'] = news['Text'].apply(lambda x: len(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5760b5fb-e693-4859-afda-6008ee745dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2042769-109c-4b20-814f-4f835fec3391",
   "metadata": {},
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ff150-17d9-40d1-8b43-6889f16bd357",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('inv' if any(news['Token_Count'] > 300) else 'ok')\n",
    "print('inv' if any(leaks['Token_Count'] > 4000) else 'ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a571b-8c76-403d-a75f-36c951166b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_counts_news = Counter(news['Text'].apply(detect))\n",
    "language_counts_leaks = Counter(leaks['Text'].apply(detect))\n",
    "print(language_counts_news)\n",
    "print(language_counts_leaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ae55a-14e4-4f9f-b40f-73f384ac29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd3115f-f9cd-4c00-bd46-a9ce33a53f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.drop(columns=['Link','Token_Count'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd31153f-1518-4584-bd11-33a40ce12d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "leaks.drop(columns=['PDF Path','Token_Count'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5071b208-b25c-4920-abcd-5936283b3698",
   "metadata": {},
   "outputs": [],
   "source": [
    "news.to_csv('news.csv', index=False)\n",
    "leaks.to_csv('leaks.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc59c3b-ef50-45ec-869a-be51fc551f67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis  # For newer versions\n",
    "import pyLDAvis.gensim_models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Transform text into a document-term matrix\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=5000)\n",
    "doc_term_matrix = vectorizer.fit_transform(news['Text'])\n",
    "\n",
    "# Fit an LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(doc_term_matrix)\n",
    "\n",
    "# Visualization for pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare the visualization\n",
    "lda_vis = pyLDAvis.prepare(\n",
    "    topic_term_dists=lda_model.components_,\n",
    "    doc_topic_dists=lda_model.transform(doc_term_matrix),\n",
    "    doc_lengths=[len(doc.split()) for doc in news['Text']],\n",
    "    vocab=vectorizer.get_feature_names_out(),\n",
    "    term_frequency=doc_term_matrix.sum(axis=0).A1\n",
    ")\n",
    "\n",
    "pyLDAvis.display(lda_vis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a83ba5b-e18d-42a0-b4ff-c3f199cdf536",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Create a document-term matrix for the 'leaks' dataset\n",
    "leaks_term_matrix = vectorizer.fit_transform(leaks['Text'])\n",
    "\n",
    "# Fit an LDA model using the 'leaks' dataset\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_model.fit(leaks_term_matrix)\n",
    "\n",
    "# Visualization for pyLDAvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# Prepare the visualization\n",
    "lda_vis = pyLDAvis.prepare(\n",
    "    topic_term_dists=lda_model.components_,\n",
    "    doc_topic_dists=lda_model.transform(leaks_term_matrix),\n",
    "    doc_lengths=[len(doc.split()) for doc in leaks['Text']],\n",
    "    vocab=vectorizer.get_feature_names_out(),\n",
    "    term_frequency=leaks_term_matrix.sum(axis=0).A1\n",
    ")\n",
    "\n",
    "pyLDAvis.display(lda_vis)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2930754c-fa9b-47d0-9787-196229153cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_files_to_s3('bia-datathon-text-data-unstructured', folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
